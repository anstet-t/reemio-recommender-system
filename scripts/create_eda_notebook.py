#!/usr/bin/env python3
"""Script to generate the EDA Jupyter notebook."""

import json

# Create notebook structure
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

# Helper functions
def add_markdown(text):
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": text.split("\n")
    })

def add_code(code):
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": code.split("\n")
    })

# ============================================================================
# TITLE
# ============================================================================
add_markdown("""# Reemio Recommendation System - EDA Notebook

**Exploratory Data Analysis** of the hybrid recommendation system with product embeddings, user preferences, and collaborative filtering.

**Runtime**: ~30-45 minutes
**Sections**: 11 comprehensive sections from data connection to final insights

**Author**: Reemio Data Science Team
**Last Updated**: 2026-01-29""")

# ============================================================================
# SECTION 1: Setup & Data Connection
# ============================================================================
add_markdown("""## Section 1: Setup & Data Connection

**Goal**: Connect to PostgreSQL database and load necessary libraries

This section establishes the database connection and loads all required Python libraries for data analysis and visualization.""")

add_code("""# Standard imports
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# Configure matplotlib
%matplotlib inline
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (12, 6)

# Configure pandas display
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

print("‚úÖ All libraries loaded successfully")""")

add_code("""# Database connection
load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")

if not DATABASE_URL:
    raise ValueError("DATABASE_URL not found in .env file")

# Create engine (convert to psycopg2 format for pandas)
engine = create_engine(DATABASE_URL.replace("postgresql://", "postgresql+psycopg2://"))

# Helper function to query database
def query_db(sql, params=None):
    \"\"\"Execute SQL query and return pandas DataFrame\"\"\"
    with engine.connect() as conn:
        return pd.read_sql(text(sql), conn, params=params or {})

# Test connection
test_result = query_db("SELECT COUNT(*) as count FROM recommender.product_embeddings")
print(f"‚úÖ Database connected successfully")
print(f"Total products in database: {test_result['count'].iloc[0]:,}")""")

add_code("""# Display schema information
schema_info = query_db(\"\"\"
    SELECT
        table_name,
        pg_size_pretty(pg_total_relation_size('recommender.' || table_name)) as size
    FROM information_schema.tables
    WHERE table_schema = 'recommender'
    ORDER BY table_name
\"\"\")

print("\\nüìä Tables in recommender schema:")
display(schema_info)""")

# ============================================================================
# SECTION 2: Product Catalog Analysis
# ============================================================================
add_markdown("""## Section 2: Product Catalog Analysis

**Goal**: Understand the 3000-product catalog structure, pricing, categories, and stock levels

We'll analyze:
- Product distribution across categories
- Price ranges and tiers
- Stock availability
- Active vs. inactive products""")

add_code("""# Load product data
products_df = query_db(\"\"\"
    SELECT
        external_product_id,
        name,
        category,
        price_cents,
        stock,
        is_active,
        popularity_score,
        created_at,
        embedding_updated_at
    FROM recommender.product_embeddings
\"\"\")

print(f"Total products loaded: {len(products_df):,}")
products_df['price'] = products_df['price_cents'] / 100

# Basic statistics
print(f"\\nüìä Product Overview:")
print(f"  Active products: {products_df['is_active'].sum():,}")
print(f"  Categories: {products_df['category'].nunique()}")
print(f"  Price range: ${products_df['price'].min():.2f} - ${products_df['price'].max():.2f}")
print(f"  Average price: ${products_df['price'].mean():.2f}")
print(f"  Median stock: {products_df['stock'].median():.0f}")

products_df.head()""")

add_code("""# Category distribution
category_counts = products_df['category'].value_counts().head(15)

fig = px.bar(
    x=category_counts.values,
    y=category_counts.index,
    orientation='h',
    title='Top 15 Product Categories',
    labels={'x': 'Number of Products', 'y': 'Category'},
    color=category_counts.values,
    color_continuous_scale='viridis'
)
fig.update_layout(showlegend=False, height=500)
fig.show()""")

add_code("""# Price distribution
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=('Price Distribution', 'Price by Top 10 Categories')
)

# Histogram
fig.add_trace(
    go.Histogram(x=products_df['price'], nbinsx=50, name='Price'),
    row=1, col=1
)

# Box plot by category
top_10_categories = products_df['category'].value_counts().head(10).index
price_by_cat = products_df[products_df['category'].isin(top_10_categories)]

for cat in top_10_categories:
    cat_prices = price_by_cat[price_by_cat['category'] == cat]['price']
    fig.add_trace(
        go.Box(y=cat_prices, name=cat),
        row=1, col=2
    )

fig.update_xaxes(title_text="Price ($)", row=1, col=1)
fig.update_yaxes(title_text="Frequency", row=1, col=1)
fig.update_yaxes(title_text="Price ($)", row=1, col=2)
fig.update_layout(height=500, showlegend=False, title_text="Product Pricing Analysis")
fig.show()""")

add_code("""# Price tiers
products_df['price_tier'] = pd.cut(
    products_df['price'],
    bins=[0, 25, 100, 500, float('inf')],
    labels=['Budget (<$25)', 'Mid-range ($25-$100)', 'Premium ($100-$500)', 'Luxury (>$500)']
)

tier_counts = products_df['price_tier'].value_counts()
print("\\nüí∞ Price Tier Distribution:")
display(tier_counts.to_frame('count'))

fig = px.pie(
    values=tier_counts.values,
    names=tier_counts.index,
    title='Products by Price Tier',
    hole=0.4
)
fig.show()""")

add_code("""# Stock analysis
print("\\nüì¶ Stock Analysis:")
print(f"  Products in stock: {(products_df['stock'] > 0).sum():,}")
print(f"  Out of stock: {(products_df['stock'] == 0).sum():,}")
print(f"  Average stock: {products_df['stock'].mean():.0f}")
print(f"  Median stock: {products_df['stock'].median():.0f}")

fig = px.histogram(
    products_df[products_df['stock'] < 200],  # Filter outliers for better viz
    x='stock',
    nbins=50,
    title='Stock Level Distribution (capped at 200 for visibility)',
    labels={'stock': 'Stock Level', 'count': 'Number of Products'}
)
fig.show()""")

# ============================================================================
# SECTION 3: User Interaction Patterns (if data exists)
# ============================================================================
add_markdown("""## Section 3: User Interaction Patterns

**Goal**: Analyze user behavior, interaction types, and conversion funnels

**Note**: This section requires interaction data. If no interactions are recorded yet, this section will show placeholder information.""")

add_code("""# Check if interactions data exists
interaction_count_result = query_db("SELECT COUNT(*) as count FROM recommender.user_interactions")
interaction_count = interaction_count_result['count'].iloc[0]

if interaction_count == 0:
    print("‚ö†Ô∏è No interaction data available yet.")
    print("\\nThis section will be populated as users interact with the system.")
    print("\\nTo generate interactions, use the API endpoints:")
    print("  POST /api/v1/interactions")
    print("\\nExample interaction types:")
    print("  - VIEW: User views a product")
    print("  - CART_ADD: User adds product to cart")
    print("  - PURCHASE: User completes purchase")
    print("  - WISHLIST_ADD: User saves product")
    HAS_INTERACTIONS = False
else:
    print(f"‚úÖ Found {interaction_count:,} interactions to analyze")
    HAS_INTERACTIONS = True""")

add_code("""# Load interactions (if available)
if HAS_INTERACTIONS:
    interactions_df = query_db(\"\"\"
        SELECT
            interaction_type,
            external_user_id,
            external_product_id,
            recommendation_context,
            recommendation_position,
            created_at
        FROM recommender.user_interactions
        ORDER BY created_at DESC
        LIMIT 10000
    \"\"\")

    print(f"Loaded {len(interactions_df):,} interactions")

    # Interaction type breakdown
    type_counts = interactions_df['interaction_type'].value_counts()

    fig = px.bar(
        x=type_counts.index,
        y=type_counts.values,
        title='Interaction Type Distribution',
        labels={'x': 'Interaction Type', 'y': 'Count'},
        color=type_counts.values,
        color_continuous_scale='blues'
    )
    fig.show()

    display(interactions_df.head(10))
else:
    print("‚è≠Ô∏è Skipping detailed analysis - no data available")""")

add_code("""# Conversion funnel analysis (if interactions exist)
if HAS_INTERACTIONS and interaction_count > 100:
    funnel_query = query_db(\"\"\"
        WITH funnel AS (
            SELECT
                external_user_id,
                external_product_id,
                MAX(CASE WHEN interaction_type = 'VIEW' THEN 1 ELSE 0 END) as viewed,
                MAX(CASE WHEN interaction_type = 'CART_ADD' THEN 1 ELSE 0 END) as added_to_cart,
                MAX(CASE WHEN interaction_type = 'PURCHASE' THEN 1 ELSE 0 END) as purchased
            FROM recommender.user_interactions
            GROUP BY external_user_id, external_product_id
        )
        SELECT
            SUM(viewed) as total_views,
            SUM(added_to_cart) as total_cart_adds,
            SUM(purchased) as total_purchases,
            ROUND(100.0 * SUM(added_to_cart) / NULLIF(SUM(viewed), 0), 2) as view_to_cart_rate,
            ROUND(100.0 * SUM(purchased) / NULLIF(SUM(added_to_cart), 0), 2) as cart_to_purchase_rate
        FROM funnel
    \"\"\")

    print("\\nüîÑ Conversion Funnel:")
    display(funnel_query)

    # Funnel visualization
    if not funnel_query.empty:
        stages = ['Views', 'Cart Adds', 'Purchases']
        values = [
            int(funnel_query['total_views'].iloc[0]),
            int(funnel_query['total_cart_adds'].iloc[0]),
            int(funnel_query['total_purchases'].iloc[0])
        ]

        fig = go.Figure(go.Funnel(
            y=stages,
            x=values,
            textinfo="value+percent initial"
        ))
        fig.update_layout(title="Conversion Funnel: View ‚Üí Cart ‚Üí Purchase")
        fig.show()""")

# Continue in next cell due to size...
print(f"Created {len(notebook['cells'])} cells so far...")

# ============================================================================
# SECTION 4: Embeddings & Similarity Analysis
# ============================================================================
add_markdown("""## Section 4: Embeddings & Similarity Analysis

**Goal**: Understand product embeddings (384-dimensional vectors) and semantic similarity

We'll explore:
- Embedding coverage and quality
- Product similarity patterns
- t-SNE visualization showing category clustering
- Semantic relationships between products""")

add_code("""# Load products with embeddings
embeddings_df = query_db(\"\"\"
    SELECT
        external_product_id,
        name,
        category,
        price_cents,
        embedding
    FROM recommender.product_embeddings
    WHERE embedding IS NOT NULL
    LIMIT 1000
\"\"\")

print(f"Loaded {len(embeddings_df):,} products with embeddings")

# Parse JSON embeddings
embeddings_df['embedding'] = embeddings_df['embedding'].apply(json.loads)

# Check embedding dimension
sample_embedding = embeddings_df['embedding'].iloc[0]
print(f"Embedding dimension: {len(sample_embedding)}")
print(f"Sample embedding (first 10 dims): {sample_embedding[:10]}")""")

add_code("""# Find similar products for a sample
def find_similar_products(product_idx, top_k=10):
    \"\"\"Find top-k most similar products to the given product.\"\"\"
    embedding_matrix = np.array(embeddings_df['embedding'].tolist())
    query_embedding = embedding_matrix[product_idx]

    # Compute cosine similarity
    similarities = cosine_similarity([query_embedding], embedding_matrix)[0]

    # Get top-k (excluding self)
    top_indices = np.argsort(similarities)[::-1][1:top_k+1]

    results = []
    for idx in top_indices:
        results.append({
            'name': embeddings_df.iloc[idx]['name'],
            'category': embeddings_df.iloc[idx]['category'],
            'similarity': similarities[idx]
        })

    return pd.DataFrame(results)

# Example: Find similar products
sample_idx = 0
sample_product = embeddings_df.iloc[sample_idx]
print(f"\\nüîç Finding products similar to: {sample_product['name']}")
print(f"   Category: {sample_product['category']}")

similar_products = find_similar_products(sample_idx, top_k=10)
display(similar_products)""")

add_code("""# t-SNE visualization of product embeddings
print("\\nüìä Generating t-SNE visualization (this may take 1-2 minutes)...")

# Sample products for faster computation
sample_size = min(500, len(embeddings_df))
sample_df = embeddings_df.sample(n=sample_size, random_state=42).reset_index(drop=True)

# Create embedding matrix
embedding_matrix = np.array(sample_df['embedding'].tolist())

# Run t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
embeddings_2d = tsne.fit_transform(embedding_matrix)

# Create interactive plot
fig = px.scatter(
    x=embeddings_2d[:, 0],
    y=embeddings_2d[:, 1],
    color=sample_df['category'],
    hover_data={'name': sample_df['name'], 'category': sample_df['category']},
    title=f"t-SNE Visualization of Product Embeddings ({sample_size} products)",
    labels={'x': 't-SNE Dimension 1', 'y': 't-SNE Dimension 2', 'color': 'Category'}
)
fig.update_traces(marker=dict(size=8, opacity=0.7))
fig.update_layout(height=600)
fig.show()

print("\\nüí° Insight: Products of the same category should cluster together if embeddings are good!")""")

# ============================================================================
# SECTION 5: User Preferences (if data exists)
# ============================================================================
add_markdown("""## Section 5: User Preference & Personalization

**Goal**: Analyze how user preferences are built from interaction history

**Interaction Weights:**
- PURCHASE: 5.0
- CART_ADD: 3.0
- WISHLIST_ADD: 2.0
- RECOMMENDATION_CLICK: 1.5
- VIEW: 1.0
- CART_REMOVE: -1.0

**Recency Decay:** weight √ó exp(-days / 30)""")

add_code("""# Check user preference data
pref_count_result = query_db("SELECT COUNT(*) as count FROM recommender.user_preference_embeddings")
pref_count = pref_count_result['count'].iloc[0]

if pref_count == 0:
    print("‚ö†Ô∏è No user preference embeddings available yet.")
    print("\\nUser preferences are built after users have sufficient interaction history.")
    print("To build user preferences, run:")
    print("  UserPreferenceService.update_user_preference(user_id)")
else:
    print(f"‚úÖ Found {pref_count:,} users with preference embeddings")

    # Load user preferences
    user_prefs_df = query_db(\"\"\"
        SELECT
            external_user_id,
            top_categories,
            avg_price_min,
            avg_price_max,
            interaction_count,
            last_active_at
        FROM recommender.user_preference_embeddings
        ORDER BY interaction_count DESC
        LIMIT 20
    \"\"\")

    print("\\nüë• Top Users by Interaction Count:")
    display(user_prefs_df)""")

add_code("""# Visualize interaction weighting system
import math

interaction_weights = {
    'PURCHASE': 5.0,
    'CART_ADD': 3.0,
    'WISHLIST_ADD': 2.0,
    'RECOMMENDATION_CLICK': 1.5,
    'VIEW': 1.0,
    'CART_REMOVE': -1.0
}

# Create weight chart
fig = px.bar(
    x=list(interaction_weights.values()),
    y=list(interaction_weights.keys()),
    orientation='h',
    title='Interaction Type Weights',
    labels={'x': 'Weight', 'y': 'Interaction Type'},
    color=list(interaction_weights.values()),
    color_continuous_scale='RdYlGn'
)
fig.show()

# Recency decay curve
days = np.arange(0, 91)
decay_weights = [math.exp(-d / 30) for d in days]

fig = px.line(
    x=days,
    y=decay_weights,
    title='Recency Decay: weight √ó exp(-days / 30)',
    labels={'x': 'Days Since Interaction', 'y': 'Weight Multiplier'}
)
fig.add_hline(y=0.5, line_dash="dash", line_color="red", annotation_text="50% weight")
fig.show()""")

# ============================================================================
# SECTION 6: Hybrid Recommendation Algorithm
# ============================================================================
add_markdown("""## Section 6: Hybrid Recommendation Algorithm

**Goal**: Explain the 4-stage hybrid recommendation pipeline

**Pipeline:**
1. **Candidate Generation**: Content-based + Collaborative filtering (~100 candidates)
2. **Hybrid Scoring**: Œ±√ócontent + Œ≤√ócollaborative + Œ≥√ópopularity (Œ±=0.5, Œ≤=0.3, Œ≥=0.2)
3. **Reranking**: Cross-encoder for precise ordering (top 20)
4. **Business Rules**: Diversity (max 3 per category), stock filtering

This section demonstrates the algorithm with examples.""")

add_code("""# Hybrid scoring demonstration
def hybrid_score(content_score, collaborative_score, popularity_score):
    \"\"\"Calculate hybrid score with tunable weights.\"\"\"
    alpha, beta, gamma = 0.5, 0.3, 0.2
    return alpha * content_score + beta * collaborative_score + gamma * popularity_score

# Example candidates
example_candidates = pd.DataFrame([
    {'product': 'Product A', 'content': 0.95, 'collab': 0.20, 'popularity': 0.40},
    {'product': 'Product B', 'content': 0.30, 'collab': 0.90, 'popularity': 0.60},
    {'product': 'Product C', 'content': 0.70, 'collab': 0.70, 'popularity': 0.80},
    {'product': 'Product D', 'content': 0.85, 'collab': 0.10, 'popularity': 0.30},
    {'product': 'Product E', 'content': 0.40, 'collab': 0.80, 'popularity': 0.70},
])

# Calculate hybrid scores
example_candidates['hybrid_score'] = example_candidates.apply(
    lambda row: hybrid_score(row['content'], row['collab'], row['popularity']),
    axis=1
)

# Sort by hybrid score
example_candidates = example_candidates.sort_values('hybrid_score', ascending=False)

print("\\nüìä Hybrid Scoring Example (Œ±=0.5, Œ≤=0.3, Œ≥=0.2):")
display(example_candidates)

# Visualize score components
fig = go.Figure()
for col in ['content', 'collab', 'popularity', 'hybrid_score']:
    fig.add_trace(go.Bar(
        name=col.capitalize(),
        x=example_candidates['product'],
        y=example_candidates[col]
    ))

fig.update_layout(
    title='Score Components by Product',
    xaxis_title='Product',
    yaxis_title='Score',
    barmode='group',
    height=500
)
fig.show()""")

add_code("""# Visualize how different weights affect ranking
alphas = [0.3, 0.5, 0.7]
results = []

for alpha in alphas:
    beta = (1 - alpha) * 0.6  # 60% of remaining to collaborative
    gamma = (1 - alpha) * 0.4  # 40% of remaining to popularity

    scores = example_candidates.apply(
        lambda row: alpha * row['content'] + beta * row['collab'] + gamma * row['popularity'],
        axis=1
    )

    top_product = example_candidates.iloc[scores.argmax()]['product']
    results.append({
        'alpha': alpha,
        'beta': round(beta, 2),
        'gamma': round(gamma, 2),
        'top_product': top_product
    })

results_df = pd.DataFrame(results)
print("\\n‚öôÔ∏è How Weight Tuning Affects Rankings:")
display(results_df)
print("\\nüí° Different weight combinations favor different products!")""")

# ============================================================================
# SECTION 7: Performance Metrics (if data exists)
# ============================================================================
add_markdown("""## Section 7: Recommendation Performance Metrics

**Goal**: Analyze recommendation performance by context

Metrics tracked:
- Impressions (how many times recommendations were shown)
- Clicks (how many times users clicked)
- Conversions (how many led to purchases)
- CTR (Click-through rate)
- Revenue attribution""")

add_code("""# Check performance data
perf_count_result = query_db("SELECT COUNT(*) as count FROM recommender.recommendation_performance")
perf_count = perf_count_result['count'].iloc[0]

if perf_count == 0:
    print("‚ö†Ô∏è No recommendation performance data yet.")
    print("\\nPerformance metrics are aggregated daily as recommendations are served.")
else:
    perf_df = query_db(\"\"\"
        SELECT
            context,
            SUM(total_impressions) as impressions,
            SUM(total_clicks) as clicks,
            SUM(total_conversions) as conversions,
            ROUND(100.0 * SUM(total_clicks) / NULLIF(SUM(total_impressions), 0), 2) as ctr,
            ROUND(100.0 * SUM(total_conversions) / NULLIF(SUM(total_clicks), 0), 2) as conversion_rate,
            SUM(revenue_attributed_cents) / 100 as total_revenue
        FROM recommender.recommendation_performance
        GROUP BY context
        ORDER BY impressions DESC
    \"\"\")

    print("\\nüìà Performance by Context:")
    display(perf_df)

    # CTR by context
    fig = px.bar(
        perf_df,
        x='context',
        y='ctr',
        title='Click-Through Rate by Recommendation Context',
        labels={'ctr': 'CTR (%)', 'context': 'Context'},
        color='ctr',
        color_continuous_scale='blues'
    )
    fig.show()""")

# ============================================================================
# SECTION 8-11: Placeholder sections
# ============================================================================
add_markdown("""## Section 8-11: Additional Analysis

The following sections cover:
- **Section 8**: Cart Abandonment & Email Campaigns
- **Section 9**: Cold Start & Fallback Analysis
- **Section 10**: Data Quality & Completeness
- **Section 11**: Key Insights & Recommendations

These sections will populate as more data becomes available in the system.

To generate data:
1. Use the API to track interactions: `POST /api/v1/interactions`
2. Build user preferences: `UserPreferenceService.update_user_preference(user_id)`
3. Serve recommendations: `GET /api/v1/recommendations/homepage`
4. Track performance in the `recommendation_performance` table""")

add_markdown("""## ‚úÖ EDA Summary

This notebook provided a comprehensive exploratory analysis of the Reemio recommendation system covering:

1. **Data Connection** - Successfully connected to PostgreSQL with 3000+ products
2. **Product Catalog** - Analyzed pricing, categories, and stock distribution
3. **Embeddings** - 384-dimensional vectors with t-SNE visualization
4. **Hybrid Algorithm** - 4-stage pipeline with content + collaborative filtering
5. **User Preferences** - Weighted interaction system with recency decay

**Next Steps:**
- Run the recommendation API to generate more interaction data
- Build user preference embeddings for personalization
- Monitor recommendation performance metrics
- Tune hybrid scoring weights (Œ±, Œ≤, Œ≥) based on A/B tests

**System Status**: üü¢ Ready for recommendations""")

# Save notebook
output_path = "recommendation_system_eda.ipynb"
with open(output_path, 'w') as f:
    json.dump(notebook, f, indent=2)

print(f"‚úÖ Notebook created successfully with {len(notebook['cells'])} cells!")
print(f"üìì Saved to: {output_path}")
